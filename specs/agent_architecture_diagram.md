# BioRender Figure Feedback Agent - Architecture Diagram

## Multi-Agent System Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           BIORENDER FIGURE ANALYSIS                          â”‚
â”‚                              Multi-Agent System                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INPUT: Figure Analysis Request
â”œâ”€â”€ image_data (base64)
â”œâ”€â”€ json_structure (BioRender JSON)
â”œâ”€â”€ context (optional)
â””â”€â”€ figure_type (optional)

                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  START  â”‚
                                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   CONTENT           â”‚
                            â”‚ INTERPRETATION STEP â”‚
                            â”‚  (Preprocessing)    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                    â”‚                    â”‚
                    â–¼                    â–¼                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  VISUAL DESIGN  â”‚  â”‚ COMMUNICATION   â”‚  â”‚   SCIENTIFIC    â”‚
        â”‚     AGENT       â”‚  â”‚     AGENT       â”‚  â”‚     AGENT       â”‚
        â”‚ (Content-Aware) â”‚  â”‚ (Content-Aware) â”‚  â”‚ (Content-Aware) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                        â”‚                        â”‚
                â–¼                        â–¼                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚@tool            â”‚  â”‚@tool            â”‚  â”‚@tool            â”‚
        â”‚analyze_visual_  â”‚  â”‚evaluate_comm_   â”‚  â”‚validate_sci_    â”‚
        â”‚design +         â”‚  â”‚clarity +        â”‚  â”‚accuracy +       â”‚
        â”‚content_summary  â”‚  â”‚content_summary  â”‚  â”‚content_summary  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            CONVERGENCE             â”‚
                    â”‚     (All analyses complete)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        FEEDBACK SYNTHESIZER         â”‚
                    â”‚             AGENT                   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚@tool                                â”‚
                    â”‚synthesize_feedback                  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚     END     â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTPUT: Figure Analysis Response
â”œâ”€â”€ visual_design_score (0-10)
â”œâ”€â”€ communication_score (0-10)
â”œâ”€â”€ scientific_accuracy_score (0-10)
â”œâ”€â”€ overall_score (0-30)
â”œâ”€â”€ content_summary (plain language)
â”œâ”€â”€ feedback (comprehensive analysis)
â”œâ”€â”€ recommendations (prioritized list)
â””â”€â”€ processing_time (seconds)
```

## Agent Details

### 0. Content Interpretation Step (Preprocessing)
**Function**: `content_interpretation_step()`  
**Tool**: `@tool interpret_figure_content()`  
**Purpose**: **Critical First Step** - Understand what the figure communicates before evaluating design quality
**Focus Areas**:
- ğŸ‘ï¸ Vision-based analysis (GPT-4o-mini) 
- ğŸ—£ï¸ Plain language summaries
- ğŸ”„ Hybrid approach (vision + JSON fallback)
- ğŸ“‹ Message identification for context-aware analysis

**Why Content-First?**: Agents need to know what message the figure is trying to convey before they can evaluate how well the design supports that message.

### 1. Visual Design Agent (Content-Aware)
**Function**: `visual_design_agent()`  
**Tool**: `@tool analyze_visual_design(content_summary)`  
**Focus Areas**:
- ğŸ¨ Color palette analysis (context-specific recommendations)
- ğŸ“ Typography hierarchy validation
- ğŸ“ Layout and spacing assessment  
- ğŸ’¡ Accessibility considerations
- ğŸ¯ **NEW**: Design-message alignment (e.g., pathway figures need directional flow)

**Content Integration**: References `content_summary` to provide targeted recommendations based on figure type and message.

### 2. Communication Agent (Content-Aware)
**Function**: `communication_agent()`  
**Tool**: `@tool evaluate_communication_clarity(content_summary)`  
**Focus Areas**:
- ğŸ¯ Information flow analysis
- ğŸ“Š Information density evaluation
- ğŸ‘¥ Audience appropriateness
- ğŸ’¡ Narrative clarity
- âš ï¸ **NEW**: Message-design mismatch detection

**Content Integration**: Validates that visual elements support the identified message (e.g., pathway figures should have flow indicators).

### 3. Scientific Agent (Content-Aware)
**Function**: `scientific_agent()`  
**Tool**: `@tool validate_scientific_accuracy(content_summary)`  
**Focus Areas**:
- ğŸ”¬ Nomenclature validation
- ğŸ§¬ Pathway logic verification
- ğŸ“ Standards compliance
- âš ï¸ Literature cross-reference
- ğŸ¯ **NEW**: Content-specific accuracy checks

**Content Integration**: Applies specialized validation based on identified content type (pathway vs. mechanism vs. process).

### 5. Feedback Synthesizer Agent
**Function**: `feedback_synthesizer_agent()`  
**Tool**: `@tool synthesize_feedback()`  
**Focus Areas**:
- ğŸ“Š Score aggregation and analysis
- ğŸ¯ Priority ranking of recommendations
- ğŸ“ Comprehensive feedback generation
- ğŸ’¡ Implementation guidance

**Thinking Steps**:
1. Analysis review
2. Priority ranking
3. Synthesis
4. Final review

## Execution Flow

### Content-First Processing (NEW ARCHITECTURE)
- **Phase 1**: Content Interpretation Step (preprocessing) - **CRITICAL FIRST**
- **Phase 2**: 3 analysis agents execute simultaneously (now content-aware)
- **Phase 3**: Results converge at Feedback Synthesizer  
- **Phase 4**: Synthesized output to END

### Why Content-First Architecture?
**Problem Solved**: Previous architecture had agents analyzing figures without understanding what they were trying to communicate. How can you judge if visual design is effective without knowing the intended message?

**Solution**: Content interpretation happens first, providing context for all subsequent analysis.

### Real-time Progress Tracking
```
WebSocket Connection (Optional)
â”œâ”€â”€ Agent start notifications
â”œâ”€â”€ Thinking step updates
â”œâ”€â”€ Tool execution status
â”œâ”€â”€ Agent completion with confidence scores
â””â”€â”€ Final analysis completion
```

### State Management
```typescript
FigureState {
  // Input data
  image_data: string
  json_structure: Dict[str, Any]
  context?: string
  figure_type?: string
  
  // Agent outputs
  visual_analysis?: string
  communication_analysis?: string
  scientific_analysis?: string
  content_interpretation?: string
  feedback_summary?: string
  quality_scores?: Dict[str, int]
  
  // Progress tracking
  agent_progress?: Dict[str, AgentProgress]
  websocket?: WebSocket
  session_id?: string
}
```

## Key Features

### Hybrid Vision-LLM Approach
- **Primary**: GPT-4o-mini vision API for image analysis
- **Fallback**: JSON-based rule analysis
- **Development**: TEST_MODE bypasses vision calls

### Error Handling & Resilience
- Graceful degradation when APIs unavailable
- JSON fallback for vision failures
- Parameter validation and sanitization
- Comprehensive logging and tracing

### Performance Optimization
- ~15 seconds total analysis time
- Parallel agent execution
- Efficient LLM model selection
- Optional Arize tracing integration

### API Endpoints
- `POST /analyze-figure` - JSON API
- `POST /analyze-figure/upload` - File upload
- `WebSocket /ws/{session_id}` - Real-time updates
- `GET /health` - Health check
- `GET /` - Frontend interface

## Technology Stack
- **Backend**: FastAPI, Python 3.10+
- **AI Framework**: LangGraph, LangChain
- **LLM**: OpenAI GPT-4o-mini / OpenRouter
- **Vision**: GPT-4o-mini vision API
- **WebSocket**: Real-time progress updates
- **Observability**: Arize/OpenInference (optional)
